# ü§ñ Guia de Modelos LLM - DataHawk

## üìã Vis√£o Geral

O DataHawk agora suporta modelos LLM menores e mais est√°veis, otimizados para rodar em m√°quinas com recursos limitados. Este guia explica como escolher, baixar e usar os modelos recomendados.

## üéØ Modelos Recomendados

### üìä Tabela Comparativa

| Modelo             | Tamanho | RAM M√≠n. | Velocidade | Qualidade  | Melhor Para                                |
| ------------------ | ------- | -------- | ---------- | ---------- | ------------------------------------------ |
| **TinyLlama 1.1B** | 0.8GB   | 2GB      | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê     | Valida√ß√µes simples, m√°quinas com pouca RAM |
| **Qwen 1.8B**      | 1.2GB   | 3GB      | ‚≠ê‚≠ê‚≠ê‚≠ê   | ‚≠ê‚≠ê‚≠ê‚≠ê   | Racioc√≠nio num√©rico, compara√ß√µes de dados  |
| **Gemma 2B**       | 1.5GB   | 3.5GB    | ‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Texto em portugu√™s, valida√ß√µes complexas   |
| **Phi-3 Mini**     | 2.7GB   | 4GB      | ‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Qualidade superior geral                   |

### üí° Como Escolher

**Baseado na RAM dispon√≠vel:**

```bash
# Verificar RAM dispon√≠vel
free -h

# <= 4GB RAM total ‚Üí TinyLlama ou Qwen
# 4-6GB RAM ‚Üí Gemma 2B
# >= 6GB RAM ‚Üí Phi-3 Mini (recomendado)
```

**Baseado no tipo de valida√ß√£o:**

- **Dados num√©ricos/financeiros** ‚Üí Qwen 1.8B
- **Texto em portugu√™s** ‚Üí Gemma 2B
- **Dados mistos/gerais** ‚Üí Phi-3 Mini
- **Performance m√°xima** ‚Üí TinyLlama

## üì• Download e Instala√ß√£o

### M√©todo 1: Script Autom√°tico (Recomendado)

```bash
# Tornar script execut√°vel
chmod +x scripts/download-recommended-models.sh

# Executar script interativo
./scripts/download-recommended-models.sh

# Escolher op√ß√£o:
# 1 = TinyLlama (0.8GB)
# 2 = Qwen 1.8B (1.2GB)
# 3 = Gemma 2B (1.5GB)
# 4 = Phi-3 Mini (2.7GB)
# 5 = Todos os modelos
```

### M√©todo 2: Download Manual

```bash
# Criar diret√≥rio de modelos
mkdir -p models

# TinyLlama 1.1B (ultra r√°pido)
wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -O models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

# Qwen 1.8B (racioc√≠nio num√©rico)
wget https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q4_k_m.gguf -O models/qwen1.5-1.8b-chat.Q4_K_M.gguf

# Gemma 2B (portugu√™s)
wget https://huggingface.co/google/gemma-2b-it-GGUF/resolve/main/gemma-2b-it.Q4_K_M.gguf -O models/gemma-2b-it.Q4_K_M.gguf

# Phi-3 Mini (qualidade superior)
wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf -O models/phi-3-mini-4k-instruct.Q4_K_M.gguf
```

## üöÄ Uso e Configura√ß√£o

### Inicializa√ß√£o Autom√°tica

```bash
# Script completo de inicializa√ß√£o
./start-datahawk.sh

# Verifica modelos, inicia servi√ßos LLM e OCR
# Valida configura√ß√£o automaticamente
```

### Inicializa√ß√£o Manual

```bash
# 1. Iniciar servidor LLM (seleciona melhor modelo automaticamente)
python3 llm-server-production.py

# 2. Verificar status
curl http://localhost:8000/health

# 3. Ver modelos dispon√≠veis
curl http://localhost:8000/models
```

### Teste R√°pido

```bash
# Testar valida√ß√£o simples
node dist/main.js validate \
  --input data/sample.csv \
  --config config/complete-validation.yaml \
  --output tests/test-modelo-novo \
  --format json,html
```

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### Configura√ß√£o por Modelo

O servidor seleciona automaticamente o melhor modelo, mas voc√™ pode for√ßar um espec√≠fico:

```yaml
# llm-production.yaml
llm:
  # For√ßar modelo espec√≠fico (opcional)
  force_model: "gemma-2b" # ou tinyllama, qwen-1.8b, phi3-mini

  settings:
    # Configura√ß√µes otimizadas para modelos pequenos
    context_size: 2048 # Reduzido para estabilidade
    batch_size: 128 # Batch menor
    threads: 3 # Threads limitadas
    temperature: 0.1 # Mais determin√≠stico
    max_tokens: 10 # Respostas curtas
```

### Configura√ß√£o de Mem√≥ria

```python
# No llm-server-production.py, o servidor ajusta automaticamente:
# - TinyLlama: 2 threads, batch 64
# - Qwen 1.8B: 2 threads, batch 128
# - Gemma 2B: 3 threads, batch 128
# - Phi-3 Mini: 3 threads, batch 128
```

## üìä Performance e Benchmarks

### Tempos de Resposta T√≠picos

| Modelo     | Valida√ß√£o Simples | Valida√ß√£o Complexa | CPU Load |
| ---------- | ----------------- | ------------------ | -------- |
| TinyLlama  | ~50ms             | ~200ms             | 15-25%   |
| Qwen 1.8B  | ~100ms            | ~400ms             | 25-40%   |
| Gemma 2B   | ~150ms            | ~600ms             | 30-50%   |
| Phi-3 Mini | ~200ms            | ~800ms             | 40-60%   |

### Qualidade de Valida√ß√£o

```
Teste com 100 valida√ß√µes mistas:

TinyLlama 1.1B:
‚úÖ Acertos: 78/100 (78%)
‚ö° Tempo m√©dio: 80ms

Qwen 1.8B:
‚úÖ Acertos: 85/100 (85%)
‚ö° Tempo m√©dio: 120ms

Gemma 2B:
‚úÖ Acertos: 92/100 (92%)
‚ö° Tempo m√©dio: 180ms

Phi-3 Mini:
‚úÖ Acertos: 96/100 (96%)
‚ö° Tempo m√©dio: 250ms
```

## üõ†Ô∏è Troubleshooting

### Problemas Comuns

**1. Modelo n√£o encontrado**

```bash
‚ùå Erro: Model file not found
üí° Solu√ß√£o: ./scripts/download-recommended-models.sh
```

**2. Falta de mem√≥ria**

```bash
‚ùå Erro: OOM (Out of Memory)
üí° Solu√ß√£o: Use modelo menor (TinyLlama ou Qwen)
```

**3. Servidor n√£o responde**

```bash
‚ùå Erro: Connection refused
üí° Solu√ß√£o: python3 llm-server-production.py
```

### Logs e Debugging

```bash
# Verificar logs do servidor LLM
tail -f logs/llm-server-production.log

# Verificar modelos carregados
curl http://localhost:8000/models | jq '.'

# Teste de valida√ß√£o direta
curl -X POST http://localhost:8000/validate \
  -H "Content-Type: application/json" \
  -d '{"csv_value": "S√£o Paulo", "web_value": "Sao Paulo", "field_type": "text"}'
```

### Performance Tuning

**Para m√°quinas com pouca RAM (< 4GB):**

```python
# Usar apenas TinyLlama
# Configura√ß√µes ultra conservadoras
n_ctx = 1024        # Contexto menor
n_threads = 1       # Single thread
n_batch = 32        # Batch m√≠nimo
```

**Para m√°quinas potentes (> 8GB):**

```python
# Usar Phi-3 Mini
# Configura√ß√µes otimizadas
n_ctx = 4096        # Contexto maior
n_threads = 4       # Multi-thread
n_batch = 256       # Batch maior
```

## üîÑ Migra√ß√£o de Modelos Antigos

### Do Llama-3 8B para Modelos Pequenos

```bash
# 1. Parar servidor antigo
pkill -f llm-server

# 2. Atualizar configura√ß√µes
./scripts/update-llm-config.sh

# 3. Baixar novo modelo
./scripts/download-recommended-models.sh

# 4. Recompilar
npm run build

# 5. Iniciar novo servidor
./start-datahawk.sh
```

### Compara√ß√£o de Performance

```
Llama-3 8B (antigo):
üì¶ Tamanho: 4.6GB
üíæ RAM necess√°ria: 8GB+
‚ö° Tempo de resposta: 2-5s
üõ†Ô∏è Estabilidade: ‚≠ê‚≠ê

Phi-3 Mini (novo):
üì¶ Tamanho: 2.7GB
üíæ RAM necess√°ria: 4GB
‚ö° Tempo de resposta: 200-800ms
üõ†Ô∏è Estabilidade: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```

## üìö Exemplos Pr√°ticos

### Valida√ß√£o de CPF

```bash
# Melhor modelo: Qwen 1.8B (racioc√≠nio num√©rico)
curl -X POST http://localhost:8000/validate \
  -H "Content-Type: application/json" \
  -d '{
    "csv_value": "123.456.789-01",
    "web_value": "12345678901",
    "field_type": "cpf"
  }'
```

### Valida√ß√£o de Endere√ßo

```bash
# Melhor modelo: Gemma 2B (portugu√™s)
curl -X POST http://localhost:8000/validate \
  -H "Content-Type: application/json" \
  -d '{
    "csv_value": "Rua das Flores, 123",
    "web_value": "R. das Flores 123",
    "field_type": "address"
  }'
```

### Valida√ß√£o de Empresa

```bash
# Melhor modelo: Phi-3 Mini (geral)
curl -X POST http://localhost:8000/validate \
  -H "Content-Type: application/json" \
  -d '{
    "csv_value": "Banco do Brasil S.A.",
    "web_value": "BB - Banco do Brasil",
    "field_type": "company"
  }'
```

## üéØ Pr√≥ximos Passos

1. **Escolher modelo baseado nos seus dados**
2. **Baixar com script autom√°tico**
3. **Testar com dados reais**
4. **Ajustar configura√ß√µes se necess√°rio**
5. **Monitorar performance em produ√ß√£o**

## üìû Suporte

- **Logs**: `logs/llm-server-production.log`
- **Configura√ß√£o**: `llm-production.yaml`
- **Scripts**: `scripts/` (download, update, start)
- **Documenta√ß√£o**: Este arquivo

---

**üí° Dica**: Comece sempre com o Phi-3 Mini se voc√™ tem pelo menos 4GB de RAM. √â o melhor equil√≠brio entre qualidade e performance.
