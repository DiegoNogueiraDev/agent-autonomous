# BUG-002: LLM utilizando fallback em vez de análise semântica

## 📋 Informações Básicas
- **ID:** BUG-002
- **Data:** 2025-07-21
- **Severidade:** Medium
- **Status:** Open

## 🔍 Descrição
O servidor LLM está retornando respostas de "Comparação de string fallback" em vez de realizar análise semântica adequada dos dados. Isso indica que o modelo não está sendo utilizado corretamente para validação inteligente.

## 📂 Massa de Dados
- **Teste 1:** csv_value="John Doe", web_value="Example Domain"
- **Teste 2:** csv_value="Example Domain", web_value="Example Domain"

## 🔄 Passos para Reproduzir
1. Executar: `curl -X POST http://localhost:8000/validate -H "Content-Type: application/json" -d '{"csv_value": "John Doe", "web_value": "Example Domain", "field_type": "string", "field_name": "name"}'`
2. Observar resposta com reasoning: "Comparação de string fallback"

## 📋 Resultado Esperado
- LLM deveria fazer análise semântica dos valores
- Reasoning deveria explicar a lógica da comparação
- Deveria considerar contexto e significado dos campos

## ❌ Resultado Atual
- **Teste 1:** confidence=0.5, match=false, reasoning="Comparação de string fallback"
- **Teste 2:** confidence=1.0, match=true, reasoning="Comparação de string fallback"
- Ambos os casos utilizam fallback em vez de LLM

## 📝 Logs/Evidências
```json
// Teste 1 - Valores diferentes
{
  "confidence": 0.5,
  "match": false,
  "processing_time": 6.443647146224976,
  "reasoning": "Comparação de string fallback",
  "tokens": 1
}

// Teste 2 - Valores iguais  
{
  "confidence": 1.0,
  "match": true,
  "processing_time": 6.719940423965454,
  "reasoning": "Comparação de string fallback",
  "tokens": 1
}
```

## 💡 Possíveis Causas
1. **Configuração incorreta do modelo:** LLM pode não estar carregado adequadamente
2. **Prompt inadequado:** Sistema prompt pode não estar direcionando para análise semântica
3. **Fallback ativado incorretamente:** Lógica pode estar forçando fallback
4. **Timeout muito baixo:** LLM pode não ter tempo suficiente para processar
5. **Problema na API do modelo:** Endpoint /validate pode ter bug interno

## 🔧 Solução Proposta
1. **Verificar configuração do modelo:** Confirmar que modelo está carregado corretamente
2. **Revisar system prompt:** Garantir que instruções são claras para análise semântica
3. **Aumentar timeout:** Dar mais tempo para processamento do LLM
4. **Debug do endpoint:** Adicionar logs detalhados no servidor LLM

## 📊 Impacto
- **Funcionalidade:** Media - sistema funciona mas sem inteligência
- **Qualidade:** Alta - validações são apenas comparações simples de string
- **Performance:** Baixa - não aproveita capacidade do LLM

## 🔗 Relacionado
- **BUG-001:** Problemas de conectividade LLM podem agravar este issue