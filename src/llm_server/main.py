#!/usr/bin/env python3
"""
DataHawk LLM Server - Ponto de entrada principal
Servidor Python que exp√µe o modelo Llama-3 8B via HTTP para o Node.js
"""

import sys
from pathlib import Path

# Adicionar o diret√≥rio src ao path para imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from llm_server import LlamaServer, ServerHealth, create_app, setup_signal_handlers
from llm_server.logs.logger import get_logger, get_metrics_logger


def main():
    """Fun√ß√£o principal do servidor."""
    logger = get_logger('llm_server.main')
    metrics_logger = get_metrics_logger()

    # Configurar signal handlers
    setup_signal_handlers()

    logger.info("üöÄ Iniciando DataHawk LLM Server...")
    print("üöÄ Starting DataHawk LLM Server...")
    print("üì° Endpoints:")
    print("   GET  /health - Health check")
    print("   POST /load   - Load model")
    print("   POST /generate - Generate response")
    print("   POST /completion - Llama.cpp compatible")
    print("   POST /validate - Validation-specific")
    print()
    print("üìù Logs salvos em:")
    print("   logs/llm-server.log - Log geral")
    print("   logs/llm-server-errors.log - Apenas erros")
    print("   logs/llm-server-metrics.log - M√©tricas de performance")
    print()

    # Criar inst√¢ncias principais
    server = LlamaServer()
    health_monitor = ServerHealth()

    # Log de inicializa√ß√£o
    metrics_logger.info("SERVER_START: Servidor LLM iniciado")

    try:
        # Auto-load model if it exists
        model_path = './models/llama3-8b-instruct.Q4_K_M.gguf'
        if Path(model_path).exists():
            logger.info("üîÑ Carregamento autom√°tico do modelo...")
            print("üîÑ Auto-loading model...")
            if server.load_model(model_path):
                logger.info("‚úÖ Modelo carregado automaticamente com sucesso")
            else:
                logger.error("‚ùå Falha no carregamento autom√°tico do modelo")
        else:
            logger.warning(f"‚ö†Ô∏è Arquivo do modelo n√£o encontrado: {model_path}")
            print(f"‚ö†Ô∏è Model file not found: {model_path}")
            print("   O servidor continuar√° funcionando, mas sem modelo carregado.")
            print("   Use o endpoint /load para carregar um modelo.")

        # Criar aplica√ß√£o Flask
        app = create_app(server, health_monitor)

        logger.info("üåê Servidor HTTP iniciando na porta 8000...")
        app.run(host='127.0.0.1', port=8000, debug=False, threaded=True)

    except Exception as e:
        logger.critical(f"üíÄ Falha cr√≠tica ao iniciar servidor: {e}")
        metrics_logger.error(f"SERVER_STARTUP_FAILED: {str(e)}")
        sys.exit(1)


if __name__ == '__main__':
    main()
