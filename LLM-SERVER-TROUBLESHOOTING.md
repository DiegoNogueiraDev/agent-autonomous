# üõ†Ô∏è DataHawk LLM Server - Guia de Solu√ß√£o de Problemas

## üö® Problema Identificado: Segmentation Fault

O servidor LLM est√° sendo fechado bruscamente com segmentation fault. Isso geralmente ocorre devido a:

1. **Mem√≥ria insuficiente** para carregar o modelo de 4.58GB
2. **Arquivo de modelo corrompido** ou incompleto
3. **Configura√ß√µes incompat√≠veis** do llama-cpp
4. **Conflitos de bibliotecas** Python

## ‚úÖ Solu√ß√£o Implementada

### 1. Servidor Seguro (`llm-server-safe.py`)
Criamos uma vers√£o mais robusta com:
- ‚úÖ Verifica√ß√£o de mem√≥ria antes de carregar
- ‚úÖ Timeout de opera√ß√µes
- ‚úÖ Tratamento de erros aprimorado
- ‚úÖ Garbage collection autom√°tico
- ‚úÖ Configura√ß√µes conservadoras

### 2. Diagn√≥stico Autom√°tico (`diagnose-llm.py`)
Script para identificar problemas espec√≠ficos.

## üöÄ Como Usar

### Op√ß√£o 1: Servidor Seguro (Recomendado)
```bash
# Executar o servidor seguro
python3 llm-server-safe.py

# Em outro terminal, executar testes
python3 test-llm-server-safe.py
```

### Op√ß√£o 2: Diagn√≥stico Completo
```bash
# Primeiro, instalar depend√™ncias
pip install psutil

# Executar diagn√≥stico
python3 diagnose-llm.py
```

### Op√ß√£o 3: Verifica√ß√£o Manual
```bash
# Verificar tamanho do modelo
ls -lh models/llama3-8b-instruct.Q4_K_M.gguf

# Verificar mem√≥ria dispon√≠vel
free -h

# Verificar se √© GGUF v√°lido
hexdump -C models/llama3-8b-instruct.Q4_K_M.gguf | head -1
```

## üìã Checklist de Verifica√ß√£o

### 1. Recursos do Sistema
- [ ] **RAM**: M√≠nimo 8GB (recomendado 12GB+)
- [ ] **Disco**: 6GB livres para o modelo
- [ ] **CPU**: 2+ cores dispon√≠veis

### 2. Arquivo do Modelo
- [ ] **Tamanho**: 4.5GB+ (4,580,000,000 bytes)
- [ ] **Integridade**: Header GGUF v√°lido
- [ ] **Permiss√µes**: Leitura execut√°vel

### 3. Ambiente Python
- [ ] **Python**: 3.8+
- [ ] **llama-cpp-python**: Instalado
- [ ] **flask**: Instalado
- [ ] **psutil**: Instalado (opcional mas recomendado)

## üîß Instala√ß√£o de Depend√™ncias

```bash
# Instalar pacotes necess√°rios
pip install llama-cpp-python flask psutil

# Ou via requirements.txt
pip install -r requirements.txt
```

## üìù Logs e Monitoramento

### Localiza√ß√£o dos Logs
```
logs/
‚îú‚îÄ‚îÄ llm-server.log          # Log geral
‚îú‚îÄ‚îÄ llm-server-errors.log   # Apenas erros
‚îî‚îÄ‚îÄ llm-server-metrics.log  # M√©tricas de performance
```

### Monitoramento em Tempo Real
```bash
# Verificar logs em tempo real
tail -f logs/llm-server.log

# Verificar uso de mem√≥ria
htop
```

## üéØ Configura√ß√µes de Seguran√ßa

### Limites Configurados
- **Context Size**: 4096 (reduzido de 8192)
- **Threads**: 2 (reduzido de 4)
- **Batch Size**: 256 (reduzido de 512)
- **Timeout**: 60 segundos para carregamento
- **Memory Check**: 6GB m√≠nimo

### Par√¢metros Ajust√°veis
Edite `src/llm_server/core/model_manager_safe.py`:
```python
# Ajustar conforme necess√°rio
max_memory_usage_gb = 6.0    # Reduzir se mem√≥ria for limitada
max_context_size = 4096      # Reduzir para estabilidade
max_batch_size = 256         # Reduzir para mem√≥ria
```

## üö® Erros Comuns e Solu√ß√µes

### Erro: "Out of Memory"
```bash
# Solu√ß√£o: Reduzir par√¢metros
# Editar model_manager_safe.py
max_context_size = 2048
max_batch_size = 128
```

### Erro: "Model file not found"
```bash
# Solu√ß√£o: Verificar caminho
ls -la models/
# Ou especificar caminho completo
python3 llm-server-safe.py --model-path /caminho/completo/modelo.gguf
```

### Erro: "Segmentation fault"
```bash
# Solu√ß√£o: Usar servidor seguro
python3 llm-server-safe.py
# Se persistir, verificar integridade do modelo
python3 diagnose-llm.py
```

## üìä Testes de Performance

### Teste de Carga
```bash
# Executar testes automatizados
python3 test-llm-server-safe.py
```

### Teste Manual com curl
```bash
# Health check
curl http://localhost:8000/health

# Validar dados
curl -X POST http://localhost:8000/validate \
  -H "Content-Type: application/json" \
  -d '{"csv_value":"John","web_value":"John","field_type":"name"}'
```

## üîÑ Reinicializa√ß√£o Autom√°tica

### Systemd Service (Linux)
Crie `/etc/systemd/system/datahawk-llm.service`:
```ini
[Unit]
Description=DataHawk LLM Server
After=network.target

[Service]
Type=simple
User=youruser
WorkingDirectory=/path/to/project
ExecStart=/usr/bin/python3 llm-server-safe.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

### Docker (Opcional)
```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt
EXPOSE 8000
CMD ["python3", "llm-server-safe.py"]
```

## üìû Suporte

Se os problemas persistirem:
1. Execute `python3 diagnose-llm.py` e envie o output
2. Verifique os logs em `logs/llm-server.log`
3. Teste com um modelo menor primeiro
4. Considere usar CPU com mais mem√≥ria

## üéâ Sucesso!

Ap√≥s aplicar estas solu√ß√µes, o servidor LLM deve:
- ‚úÖ Iniciar sem segmentation fault
- ‚úÖ Carregar o modelo de forma segura
- ‚úÖ Responder aos health checks
- ‚úÖ Processar requisi√ß√µes de valida√ß√£o
- ‚úÖ Manter logs detalhados para debugging
