#!/usr/bin/env node

/**
 * Teste r√°pido para verificar comunica√ß√£o com LLM Server
 */

import { LocalLLMEngine } from './dist/llm/local-llm-engine.js';

async function testLLMCommunication() {
  console.log('üß™ Testando comunica√ß√£o com LLM Server...');

  const llmEngine = new LocalLLMEngine({
    settings: {
      modelPath: './models/llama-2-7b-chat.gguf',
      contextSize: 2048,
      threads: 4,
      temperature: 0.7,
      maxTokens: 100
    }
  });

  try {
    console.log('Inicializando LLM Engine...');
    await llmEngine.initialize();

    console.log('‚úÖ LLM Engine inicializado com sucesso');

    // Teste de valida√ß√£o
    const testRequest = {
      csvValue: 'test@example.com',
      webValue: 'test@example.com',
      fieldType: 'email',
      fieldName: 'email'
    };

    console.log('Enviando requisi√ß√£o de valida√ß√£o...');
    const result = await llmEngine.makeValidationDecision(testRequest);

    console.log('\nResultado da valida√ß√£o:');
    console.log(`Match: ${result.match}`);
    console.log(`Confidence: ${result.confidence}`);
    console.log(`Reasoning: ${result.reasoning}`);
    console.log(`‚úÖ Comunica√ß√£o com LLM Server funcionando!`);

  } catch (error) {
    console.error('‚ùå Erro na comunica√ß√£o com LLM Server:', error.message);
  } finally {
    await llmEngine.cleanup();
  }
}

testLLMCommunication().catch(console.error);
