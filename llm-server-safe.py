#!/usr/bin/env python3
"""
DataHawk LLM Server - VersÃ£o Segura com ProteÃ§Ã£o contra Segmentation Fault
"""

import sys
import os
from pathlib import Path

# Adicionar o diretÃ³rio src ao path para imports
sys.path.insert(0, str(Path(__file__).parent / 'src'))

# Import seguro
try:
    from llm_server.core.model_manager_safe import SafeLlamaServer
    from llm_server.core.health_monitor import ServerHealth
    from llm_server.api.routes import create_app_safe
    from llm_server.logs.logger import get_logger, get_metrics_logger
    from llm_server.utils.signals import setup_signal_handlers
except ImportError as e:
    print(f"âŒ Erro ao importar mÃ³dulos: {e}")
    sys.exit(1)


def main():
    """FunÃ§Ã£o principal do servidor seguro."""
    logger = get_logger('llm_server.main_safe')
    metrics_logger = get_metrics_logger()

    # Configurar signal handlers
    setup_signal_handlers()

    logger.info("ğŸš€ Iniciando DataHawk LLM Server - VersÃ£o Segura...")
    print("ğŸš€ Starting DataHawk LLM Server - Safe Mode")
    print("ğŸ“¡ Endpoints:")
    print("   GET  /health - Health check")
    print("   POST /load   - Load model")
    print("   POST /generate - Generate response")
    print("   POST /completion - Llama.cpp compatible")
    print("   POST /validate - Validation-specific")
    print()
    print("ğŸ›¡ï¸  Features de seguranÃ§a:")
    print("   - ProteÃ§Ã£o contra segmentation fault")
    print("   - VerificaÃ§Ã£o de memÃ³ria")
    print("   - Timeout de operaÃ§Ãµes")
    print("   - Garbage collection automÃ¡tico")
    print()
    print("ğŸ“ Logs salvos em:")
    print("   logs/llm-server.log - Log geral")
    print("   logs/llm-server-errors.log - Apenas erros")
    print("   logs/llm-server-metrics.log - MÃ©tricas de performance")
    print()

    # Criar instÃ¢ncias principais
    server = SafeLlamaServer()
    health_monitor = ServerHealth()

    # Log de inicializaÃ§Ã£o
    metrics_logger.info("SERVER_START: Servidor LLM seguro iniciado")

    try:
        # Verificar se o modelo existe
        model_path = './models/llama3-8b-instruct.Q4_K_M.gguf'
        if Path(model_path).exists():
            logger.info("ğŸ”„ Carregamento automÃ¡tico do modelo...")
            print("ğŸ”„ Auto-loading model...")

            # Tentar carregar com verificaÃ§Ãµes de seguranÃ§a
            if server.load_model(model_path):
                logger.info("âœ… Modelo carregado automaticamente com sucesso")
                print("âœ… Model loaded successfully")
            else:
                logger.error("âŒ Falha no carregamento automÃ¡tico do modelo")
                print("âŒ Failed to load model automatically")
                print("   O servidor continuarÃ¡ sem modelo carregado.")
                print("   Use o endpoint /load para carregar manualmente.")
        else:
            logger.warning(f"âš ï¸ Arquivo do modelo nÃ£o encontrado: {model_path}")
            print(f"âš ï¸ Model file not found: {model_path}")
            print("   O servidor continuarÃ¡ funcionando, mas sem modelo carregado.")
            print("   Use o endpoint /load para carregar um modelo.")

        # Criar aplicaÃ§Ã£o Flask segura
        app = create_app_safe(server, health_monitor)

        logger.info("ğŸŒ Servidor HTTP iniciando na porta 8000...")
        print("ğŸŒ HTTP server starting on port 8000...")

        # ConfiguraÃ§Ãµes de seguranÃ§a para Flask
        app.run(
            host='127.0.0.1',
            port=8000,
            debug=False,
            threaded=True,
            use_reloader=False  # Desabilita auto-reload para estabilidade
        )

    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Servidor interrompido pelo usuÃ¡rio")
        server.shutdown()

    except Exception as e:
        logger.critical(f"ğŸ’€ Falha crÃ­tica ao iniciar servidor: {e}")
        metrics_logger.error(f"SERVER_STARTUP_FAILED: {str(e)}")
        server.shutdown()
        sys.exit(1)


if __name__ == '__main__':
    main()
